#Theoretical foundation https://habr.com/ru/post/514818/
#Есть три сходных между собой понятия: 
####
#Интерполяция — способ выбрать из семейства функций ту, которая проходит через заданные точки.
#Примеры интерполяции:
#интерполяция полиномами Лагранжа, сплайн-интерполяция, многомерная интерполяция (билинейная, трилинейная, методом ближайшего соседа и т.д). 
#Есть также родственное понятие экстраполяции — предсказание поведения функции вне интервала. Например, предсказание курса доллара на основании предыдущих колебаний — экстраполяция.
####
#Аппроксимация — способ выбрать из семейства «простых» функций приближение для «сложной» функции на отрезке, при этом ошибка не должна превышать определенного предела.
#Примеры аппроксимации:
#ряд Тейлора на отрезке, аппроксимацию ортогональными многочленами, аппроксимацию Паде, аппроксимацию синуса Бхаскара и т.п.
####
#Регрессия — способ выбрать из семейства функций ту, которая минимизирует функцию потерь.
#Примеры регрессии:
#LAD, метод наименьших квадратов, Ridge регрессия, Lasso регрессия, ElasticNet и многие другие
#В каком-то смысле регрессия — это «интерполирующая аппроксимация»: мы хотим провести кривую как можно ближе к точкам и при этом сохранить ее максимально простой 
#чтобы уловить общую тенденцию. 
#За баланс между этими противоречивыми желаниями как-раз отвечает функция потерь (в английской литературе «loss function» или «cost function»)
#
#Метод наименьших квадратов или МНК или Least Squares Method или OLS, LS
#
#Постановка задачи:
#Пусть дано некоторое множество точек и мы ищем функцию f(x) = b0 + b_n*x_n чтобы ее график ближе всего находился к точкам.
#Задача состоит в том что бы минимизировать сумму квадратов отклонений регрессанта от модели или Sum of Squared Errors (SSE) = nSum(y_i - a - b * x_i)^2
#
#
import numpy as np
from sklearn.linear_model import LinearRegression

#Данные в качестве векторов 
x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
y = np.array([5, 20, 14, 32, 22, 38])

#Экземпляр класса 
model = LinearRegression()

#Эта операция создаёт переменную model в качестве экземпляра LinearRegression. Вы можете предоставить несколько опциональных параметров классу LinearRegression:
#fit_intercept – логический (True по умолчанию) параметр, который решает, вычислять отрезок b₀ (True) или рассматривать его как равный нулю (False).
#normalize – логический (False по умолчанию) параметр, который решает, нормализовать входные переменные (True) или нет (False).
#copy_X – логический (True по умолчанию) параметр, который решает, копировать (True) или перезаписывать входные переменные (False).
#n_jobs – целое или None (по умолчанию), представляющее количество процессов, задействованных в параллельных вычислениях. None означает отсутствие процессов, при -1 используются все доступные процессоры.

model = LinearRegression().fit(x, y)
#С помощью .fit() вычисляются оптимальные значение весов b₀ и b₁, используя существующие вход и выход (x и y) в качестве аргументов. 

#Вы можете получить определения (R²) с помощью .score(), вызванной на model. R² - результат объясняемости ряда которое принимает значение от 0 до 1. 
#При этом оптимально получить значение от 0,6 до 0,8 иначе могут быть подозрения   
r_sq = model.score(x, y)
print('coefficient of determination:', r_sq)

#вывод значейний b0 и b_n
print('intercept:', model.intercept_)
print('slope:', model.coef_)

#Примерное значение b₀  показывает, что ваша модель предсказывает ответ b0 при x, равном нулю. 
#Равенство b₁ означает, что предсказанный ответ возрастает до b1 при x_1, увеличенным на единицу.

#Когда вас устроит ваша модель, вы можете использовать её для прогнозов с текущими или другими данными.
#Получите предсказанный ответ, используя .predict()

y_pred = model.predict(x)
#или
y_pred = model.intercept_ + model.coef_ * x
print('predicted response:', y_pred, sep='\n')
